{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53287e58-589c-49f1-8e5b-861ea3d98ce0",
   "metadata": {},
   "source": [
    "## WELFake  \n",
    "\n",
    "| Dataset Name + Model Type                                           | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|--------------------------------------------------------------------|----------|-------------------|----------------|----------------|  \n",
    "| WELFake Validation - CNN-LSTM (L2 Regularization)                 | 0.7739   | 0.7672            | 0.7661        | 0.7666        |  \n",
    "| WELFake Test - CNN-LSTM (L2 Regularization)                       | 0.7784   | 0.7646            | 0.7671        | 0.7658        |  \n",
    "| WELFake Validation - CNN-LSTM (L1 Regularization)                 | 0.7745   | 0.7676            | 0.7680        | 0.7678        |  \n",
    "| WELFake Test - CNN-LSTM (L1 Regularization)                       | 0.7795   | 0.7659            | 0.7702        | 0.7678        |  \n",
    "| **WELFake Validation - CNN-LSTM (No Regularization)**             | **0.9659**   | **0.9636**            | **0.9665**        | **0.9650**        |  \n",
    "| **WELFake Test - CNN-LSTM (No Regularization)**                   | **0.9652**   | **0.9605**            | **0.9664**        | **0.9633**        |  \n",
    "| WELFake Validation - CNN-LSTM (No Regularization, Multi-Head Attention) | 0.9596   | 0.9598            | 0.9568        | 0.9582        |  \n",
    "| WELFake Test - CNN-LSTM (No Regularization, Multi-Head Attention)       | 0.9617   | 0.9601            | 0.9585        | 0.9593        |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9f831-49cb-4bef-bef5-8c033f791e0e",
   "metadata": {},
   "source": [
    "## Fakeddit  \n",
    "\n",
    "| Dataset Name + Model Type                          | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|---------------------------------------------------|----------|-------------------|----------------|----------------|  \n",
    "| Fakeddit Validation - CNN-LSTM (L2 Regularization)  | 0.6917   | 0.6939            | 0.6740        | 0.6746        |  \n",
    "| Fakeddit Test - CNN-LSTM (L2 Regularization)        | 0.6902   | 0.6915            | 0.6716        | 0.6723        |  \n",
    "| Fakeddit Validation - CNN-LSTM (L1 Regularization)  | 0.6925   | 0.6961            | 0.6739        | 0.6743        |  \n",
    "| Fakeddit Test - CNN-LSTM (L1 Regularization)        | 0.6906   | 0.6933            | 0.6710        | 0.6715        |  \n",
    "| Fakeddit Validation - CNN-LSTM (No Regularization)  | 0.8489   | 0.8464            | 0.8492        | 0.8475        |  \n",
    "| **Fakeddit Test - CNN-LSTM (No Regularization)**        | **0.8482**   | **0.8454**            | **0.8486**        | **0.8466**        |  \n",
    "| **Fakeddit Validation - CNN-LSTM (No Regularization, Multi-Head Attention)** | **0.8497** | **0.8473** | **0.8484** | **0.8478** |  \n",
    "| Fakeddit Test - CNN-LSTM (No Regularization, Multi-Head Attention)       | 0.8480 | 0.8453 | 0.8468 | 0.8460 |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db99e5-c394-4724-8eb9-2827fe6fa76e",
   "metadata": {},
   "source": [
    "## Constraint  \n",
    "\n",
    "| Dataset Name + Model Type                          | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|---------------------------------------------------|----------|-------------------|----------------|----------------|  \n",
    "| Constraint Validation - CNN-LSTM (L2 Regularization)  | 0.9084   | 0.9081            | 0.9083        | 0.9082        |  \n",
    "| Constraint Test - CNN-LSTM (L2 Regularization)        | 0.9222   | 0.9222            | 0.9218        | 0.9220        |  \n",
    "| Constraint Validation - CNN-LSTM (L1 Regularization)  | 0.8994   | 0.9002            | 0.8983        | 0.8990        |  \n",
    "| Constraint Test - CNN-LSTM (L1 Regularization)        | 0.9081   | 0.9090            | 0.9069        | 0.9077        |  \n",
    "| Constraint Validation - CNN-LSTM (No Regularization)  | 0.9159   | 0.9158            | 0.9155        | 0.9156        |  \n",
    "| Constraint Test - CNN-LSTM (No Regularization)        | 0.9303   | 0.9303            | 0.9298        | 0.9300        |  \n",
    "| **Constraint Validation - CNN-LSTM (No Regularization, Multi-Head Attention)** | **0.9201** | **0.9202** | **0.9196** | **0.9199** |  \n",
    "| **Constraint Test - CNN-LSTM (No Regularization, Multi-Head Attention)**       | **0.9312** | **0.9312** | **0.9308** | **0.9310** |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966eadad-c1bd-41fa-a453-f3998a437163",
   "metadata": {},
   "source": [
    "## PolitiFact  \n",
    "\n",
    "| Dataset Name + Model Type                         | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|--------------------------------------------------|----------|-------------------|---------------|---------------|  \n",
    "| PolitiFact Validation - CNN-LSTM (L2 Regularization) | 0.7596   | 0.7578            | 0.7486        | 0.7513        |  \n",
    "| PolitiFact Test - CNN-LSTM (L2 Regularization)     | 0.6270   | 0.6704            | 0.6344        | 0.6090        |  \n",
    "| PolitiFact Validation - CNN-LSTM (L1 Regularization) | 0.7788   | 0.7811            | 0.7655        | 0.7694        |  \n",
    "| PolitiFact Test - CNN-LSTM (L1 Regularization)     | 0.6190   | 0.6637            | 0.6267        | 0.5993        |  \n",
    "| PolitiFact Validation - CNN-LSTM (No Regularization) | 0.5769   | 0.6661            | 0.6166        | 0.5571        |  \n",
    "| PolitiFact Test - CNN-LSTM (No Regularization)     | 0.6270   | 0.6572            | 0.6193        | 0.5995        |  \n",
    "| **PolitiFact Validation - CNN-LSTM (No Regularization, Multi-Head Attention)** | **0.8462** | **0.8618** | **0.8618** | **0.8462** |  \n",
    "| **PolitiFact Test - CNN-LSTM (No Regularization, Multi-Head Attention)**       | **0.8571** | **0.8675** | **0.8545** | **0.8554** |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17623f2c-5e6e-4fe8-a96d-634514acd72a",
   "metadata": {},
   "source": [
    "## GossipCop  \n",
    "\n",
    "| Dataset Name + Model Type                         | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|--------------------------------------------------|----------|-------------------|---------------|---------------|  \n",
    "| GossipCop Validation - CNN-LSTM (L2 Regularization) | 0.7757   | 0.6933            | 0.5419        | 0.5242        |  \n",
    "| GossipCop Test - CNN-LSTM (L2 Regularization)     | 0.7721   | 0.6872            | 0.5428        | 0.5256        |  \n",
    "| GossipCop Validation - CNN-LSTM (L1 Regularization) | 0.7749   | 0.6909            | 0.5391        | 0.5192        |  \n",
    "| GossipCop Test - CNN-LSTM (L1 Regularization)     | 0.7721   | 0.6897            | 0.5410        | 0.5220        |  \n",
    "| GossipCop Validation - CNN-LSTM (No Regularization) | 0.7749   | 0.7023            | 0.5334        | 0.5075        |  \n",
    "| GossipCop Test - CNN-LSTM (No Regularization)     | 0.7734   | 0.7110            | 0.5368        | 0.5126        |  \n",
    "| **GossipCop Validation - CNN-LSTM (No Regularization, Multi-Head Attention)** | **0.8479** | **0.8007** | **0.7455** | **0.7666** |  \n",
    "| **GossipCop Test - CNN-LSTM (No Regularization, Multi-Head Attention)**     | **0.8378** | **0.7845** | **0.7373** | **0.7557** |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff0c154-ec33-4925-93fb-1015302601f7",
   "metadata": {},
   "source": [
    "## All-Four Combined  \n",
    "\n",
    "| Dataset Name + Model Type                            | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|-----------------------------------------------------|----------|-------------------|---------------|---------------|  \n",
    "| All-Four Combined Validation - CNN-LSTM (L2 Regularization) | 0.7779   | 0.7003            | 0.5496        | 0.5377        |  \n",
    "| All-Four Combined Test - CNN-LSTM (L2 Regularization)     | 0.7734   | 0.6878            | 0.5501        | 0.5386        |  \n",
    "| All-Four Combined Validation - CNN-LSTM (L1 Regularization) | 0.7764   | 0.7061            | 0.5390        | 0.5179        |  \n",
    "| All-Four Combined Test - CNN-LSTM (L1 Regularization)     | 0.7718   | 0.6887            | 0.5399        | 0.5200        |  \n",
    "| **All-Four Combined Validation - CNN-LSTM (No Regularization)** | **0.8354**   | **0.7746**            | **0.7407**        | **0.7549**        |  \n",
    "| All-Four Combined Test - CNN-LSTM (No Regularization)     | 0.8256   | 0.7612            | 0.7321        | 0.7444        |  \n",
    "| All-Four Combined Validation - CNN-LSTM (No Regularization, Multi-Head Attention) | 0.8319   | 0.7690            | 0.7362        | 0.7499        |  \n",
    "| **All-Four Combined Test - CNN-LSTM (No Regularization, Multi-Head Attention)**     | **0.8299**   | **0.7677**            | **0.7395**        | **0.7515**        |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23551a2a-4409-4ad1-be1e-ce1a297d7520",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Best Results: Full-Scale Evaluation Scenario  \n",
    "\n",
    "| Dataset Name                                           | Best Model                              | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|--------------------------------------------------------|-----------------------------------------|----------|-------------------|----------------|----------------|  \n",
    "| WELFake Validation                                     | No Regularization                       | 0.9659   | 0.9636            | 0.9665         | 0.9650         |  \n",
    "| WELFake Test                                           | No Regularization                       | 0.9652   | 0.9605            | 0.9664         | 0.9633         |  \n",
    "| Fakeddit Validation                                    | No Regularization, Multi-Head Attention                       | 0.8497   | 0.8473            | 0.8484         | 0.8478         |  \n",
    "| Fakeddit Test                                          | No Regularization                       | 0.8482   | 0.8454            | 0.8486         | 0.8466         |  \n",
    "| Constraint Validation                                  | No Regularization, Multi-Head Attention                       | 0.9201   | 0.9202            | 0.9196         | 0.9199        |  \n",
    "| Constraint Test                                        | No Regularization, Multi-Head Attention                       | 0.9312   | 0.9312            | 0.9308         | 0.9310        |  \n",
    "| PolitiFact Validation                                  | No Regularization, Multi-Head Attention | 0.8462   | 0.8618            | 0.8618         | 0.8462         |  \n",
    "| PolitiFact Test                                        | No Regularization, Multi-Head Attention | 0.8571   | 0.8675            | 0.8545         | 0.8554         |  \n",
    "| GossipCop Validation                                   | No Regularization, Multi-Head Attention | 0.8479   | 0.8007            | 0.7455         | 0.7666         |  \n",
    "| GossipCop Test                                         | No Regularization, Multi-Head Attention | 0.8378   | 0.7845            | 0.7373         | 0.7557         |  \n",
    "| All-Four Combined Validation                           | No Regularization                       | 0.8354   | 0.7746            | 0.7407         | 0.7549         |  \n",
    "| All-Four Combined Test                                 | No Regularization, Multi-Head Attention                       | 0.8299   | 0.7677            | 0.7395         | 0.7515         |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf6b7a6-75df-4063-a64b-b9a4ebd6e223",
   "metadata": {},
   "source": [
    "## Five-Shot: WELFake Target\n",
    "\n",
    "| Dataset Name + Model Type                          | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|---------------------------------------------------|----------|-------------------|---------------|---------------|  \n",
    "| Five-Shot: WELFake Target Validation - CNN-LSTM (L2 Regularization) | 0.6109   | 0.6817            | 0.5344        | 0.4528        |  \n",
    "| Five-Shot: WELFake Target Test - CNN-LSTM (L2 Regularization)     | 0.6367   | 0.6747            | 0.5264        | 0.4488        |  \n",
    "| Five-Shot: WELFake Target Validation - CNN-LSTM (L1 Regularization) | 0.6098   | 0.6699            | 0.5336        | 0.4530        |  \n",
    "| Five-Shot: WELFake Target Test - CNN-LSTM (L1 Regularization)     | 0.6340   | 0.6479            | 0.5244        | 0.4480        |  \n",
    "| Five-Shot: WELFake Target Validation - CNN-LSTM (No Regularization) | 0.5778   | 0.5493            | 0.5407        | 0.5332        |  \n",
    "| Five-Shot: WELFake Target Test - CNN-LSTM (No Regularization)     | 0.5841   | 0.5361            | 0.5302        | 0.5256        |  \n",
    "| **Five-Shot: WELFake Target Validation - CNN-LSTM (No Regularization, Multi-Head Attention)** | **0.6117**   | **0.5959**            | **0.5588**        | **0.5360**        |  \n",
    "| **Five-Shot: WELFake Target Test - CNN-LSTM (No Regularization, Multi-Head Attention)**     | **0.6328**   | **0.5940**            | **0.5574**        | **0.5412**        |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea1780a-e610-4904-b632-1103eec9a201",
   "metadata": {},
   "source": [
    "## Five-Shot: Constraint Target\n",
    "\n",
    "| Dataset Name + Model Type                          | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|---------------------------------------------------|----------|-------------------|---------------|---------------|  \n",
    "| Five-Shot: Constraint Target Validation - CNN-LSTM (L2 Regularization) | 0.5738   | 0.5729            | 0.5730        | 0.5729        |  \n",
    "| Five-Shot: Constraint Target Test - CNN-LSTM (L2 Regularization)     | 0.5707   | 0.5697            | 0.5697        | 0.5697        |  \n",
    "| **Five-Shot: Constraint Target Validation - CNN-LSTM (L1 Regularization)** | **0.5761**   | **0.575**            | **0.5753**        | **0.5753**        |  \n",
    "| **Five-Shot: Constraint Target Test - CNN-LSTM (L1 Regularization)**     | **0.5726**   | **0.5714**            | **0.5713**        | **0.5714**        |  \n",
    "| Five-Shot: Constraint Target Validation - CNN-LSTM (No Regularization) | 0.5216   | 0.5748            | 0.5387        | 0.4658        |  \n",
    "| Five-Shot: Constraint Target Test - CNN-LSTM (No Regularization)     | 0.5113   | 0.5564            | 0.5291        | 0.4545        |  \n",
    "| Five-Shot: Constraint Target Validation - CNN-LSTM (No Regularization, Multi-Head Attention) | 0.5653   | 0.5846            | 0.5742        | 0.5549        |  \n",
    "| Five-Shot: Constraint Target Test - CNN-LSTM (No Regularization, Multi-Head Attention)     | 0.5485   | 0.5683            | 0.5584        | 0.5358        |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65d33cf-683e-4c19-a55f-3f7f4735a153",
   "metadata": {},
   "source": [
    "## Five-Shot: PolitiFact Target\n",
    "\n",
    "| Dataset Name + Model Type                          | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|---------------------------------------------------|----------|-------------------|---------------|---------------|  \n",
    "| Five-Shot: PolitiFact Target Validation - CNN-LSTM (L2 Regularization) | 0.7500   | 0.7738            | 0.7243        | 0.7273        |  \n",
    "| Five-Shot: PolitiFact Target Test - CNN-LSTM (L2 Regularization)     | 0.6111   | 0.6735            | 0.6201        | 0.5825        |  \n",
    "| Five-Shot: PolitiFact Target Validation - CNN-LSTM (L1 Regularization) | 0.7308   | 0.7681            | 0.6994        | 0.6986        |  \n",
    "| Five-Shot: PolitiFact Target Test - CNN-LSTM (L1 Regularization)     | 0.6111   | 0.6961            | 0.6211        | 0.5744        |  \n",
    "| **Five-Shot: PolitiFact Target Validation - CNN-LSTM (No Regularization)** | **0.7596**   | **0.7815**            | **0.7354**        | **0.7393**        |  \n",
    "| Five-Shot: PolitiFact Target Test - CNN-LSTM (No Regularization)     | 0.6190   | 0.6713            | 0.6272        | 0.5962        |  \n",
    "| Five-Shot: PolitiFact Target Validation - CNN-LSTM (No Regularization, Multi-Head Attention) | 0.5962   | 0.6036            | 0.6045        | 0.5960        |  \n",
    "| **Five-Shot: PolitiFact Target Test - CNN-LSTM (No Regularization, Multi-Head Attention)**     | **0.6349**   | **0.6382**            | **0.6315**        | **0.6289**        |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bb5f9-b595-4e1c-a6ce-60edd52bb521",
   "metadata": {},
   "source": [
    "## Five-Shot: GossipCop Target\n",
    "\n",
    "| Dataset Name + Model Type                          | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|---------------------------------------------------|----------|-------------------|---------------|---------------|  \n",
    "| **Five-Shot: GossipCop Target Validation - CNN-LSTM (L2 Regularization)** | **0.4829**   | **0.5540**            | **0.5708**        | **0.4724**        |  \n",
    "| Five-Shot: GossipCop Target Test - CNN-LSTM (L2 Regularization)     | 0.4817   | 0.5447            | 0.5587        | 0.4699        |  \n",
    "| Five-Shot: GossipCop Target Validation - CNN-LSTM (L1 Regularization) | 0.3745   | 0.5428            | 0.5400        | 0.3743        |  \n",
    "| Five-Shot: GossipCop Target Test - CNN-LSTM (L1 Regularization)     | 0.3839   | 0.5447            | 0.5428        | 0.3837        |  \n",
    "| Five-Shot: GossipCop Target Validation - CNN-LSTM (No Regularization) | 0.3700   | 0.4985            | 0.4984        | 0.3695        |  \n",
    "| Five-Shot: GossipCop Target Test - CNN-LSTM (No Regularization)     | 0.3926   | 0.5093            | 0.5105        | 0.3913        |  \n",
    "| Five-Shot: GossipCop Target Validation - CNN-LSTM (No Regularization, Multi-Head Attention) | 0.4916   | 0.5326            | 0.5446        | 0.4722        |  \n",
    "| **Five-Shot: GossipCop Target Test - CNN-LSTM (No Regularization, Multi-Head Attention)**     | **0.4998**   | **0.5340**            | **0.5464**        | **0.4789**        |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5a63e-4f7f-413c-aaae-be291fbeecb3",
   "metadata": {},
   "source": [
    "## Best Results: Five-Shot Evaluation Scenario\n",
    "\n",
    "| Dataset Name                         | Best Model                              | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|--------------------------------------|-----------------------------------------|----------|-------------------|----------------|----------------|  \n",
    "| Five-Shot: WELFake Target Validation | No Regularization, Multi-Head Attention | 0.6117   | 0.5959            | 0.5588         | 0.5360         |  \n",
    "| Five-Shot: WELFake Target Test       | No Regularization, Multi-Head Attention | 0.6328   | 0.5940            | 0.5574         | 0.5412         |  \n",
    "| Five-Shot: Constraint Target Validation | L1 Regularization | 0.5761   | 0.5752            | 0.5753         | 0.5753         |  \n",
    "| Five-Shot: Constraint Target Test    | L1 Regularization | 0.5726   | 0.5714            | 0.5713         | 0.5714         |  \n",
    "| Five-Shot: PolitiFact Target Validation | No Regularization | 0.7596   | 0.7815            | 0.7354         | 0.7393         |  \n",
    "| Five-Shot: PolitiFact Target Test    | No Regularization, Multi-Head Attention | 0.6349   | 0.6382            | 0.6315         | 0.6289         |  \n",
    "| Five-Shot: GossipCop Target Validation | L2 Regularization | 0.4829   | 0.5540            | 0.5708         | 0.4724         |  \n",
    "| Five-Shot: GossipCop Target Test     | No Regularization, Multi-Head Attention | 0.4998   | 0.5340            | 0.5464         | 0.4789         |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a654f5-6a8c-40b7-87b9-97fcd3f508e2",
   "metadata": {},
   "source": [
    "## Zero-Shot: WELFake Target Dataset  \n",
    "\n",
    "| Dataset Name + Model Type                          | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|---------------------------------------------------|----------|-------------------|---------------|---------------|  \n",
    "| Zero-Shot: WELFake Target Validation - CNN-LSTM (L2 Regularization) | 0.6076   | 0.7112            | 0.5284        | 0.4356        |  \n",
    "| Zero-Shot: WELFake Target Test - CNN-LSTM (L2 Regularization)     | 0.6353   | 0.7097            | 0.5219        | 0.4348        |  \n",
    "| Zero-Shot: WELFake Target Validation - CNN-LSTM (L1 Regularization) | 0.6079   | 0.7054            | 0.5291        | 0.4377        |  \n",
    "| Zero-Shot: WELFake Target Test - CNN-LSTM (L1 Regularization)     | 0.6359   | 0.7105            | 0.5228        | 0.4368        |  \n",
    "| **Zero-Shot: WELFake Target Validation - CNN-LSTM (No Regularization)** | **0.5817**   | **0.5684**            | **0.5682**        | **0.5683**        |  \n",
    "| **Zero-Shot: WELFake Target Test - CNN-LSTM (No Regularization)**     | **0.5926**   | **0.5711**            | **0.5725**        | **0.5715**        |  \n",
    "| Zero-Shot: WELFake Target Validation - CNN-LSTM (No Regularization, Multi-Head Attention) | 0.6056   | 0.5835            | 0.5620        | 0.5504        |  \n",
    "| Zero-Shot: WELFake Target Test - CNN-LSTM (No Regularization, Multi-Head Attention)     | 0.6275   | 0.5882            | 0.5673        | 0.5623        |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5961158-174e-4440-8b90-f01191e300e1",
   "metadata": {},
   "source": [
    "## Zero-Shot: Constraint Target Dataset  \n",
    "\n",
    "| Dataset Name + Model Type                          | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|---------------------------------------------------|----------|-------------------|---------------|---------------|  \n",
    "| Zero-Shot: Constraint Target Validation - CNN-LSTM (L2 Regularization) | 0.5728   | 0.5712            | 0.5668        | 0.5629        |  \n",
    "| **Zero-Shot: Constraint Target Test - CNN-LSTM (L2 Regularization)**     | **0.5895**   | **0.5887**            | **0.5834**        | **0.5800**        |  \n",
    "| Zero-Shot: Constraint Target Validation - CNN-LSTM (L1 Regularization) | 0.5771   | 0.5759            | 0.5759        | 0.5759        |  \n",
    "| **Zero-Shot: Constraint Target Validation - CNN-LSTM (L1 Regularization)** | **0.5771**   | **0.5759**            | **0.5759**        | **0.5759**        |  \n",
    "| Zero-Shot: Constraint Target Test - CNN-LSTM (L1 Regularization)     | 0.5759   | 0.5745            | 0.5744        | 0.5744        |  \n",
    "| Zero-Shot: Constraint Target Validation - CNN-LSTM (No Regularization) | 0.5602   | 0.5576            | 0.5547        | 0.5515        |  \n",
    "| Zero-Shot: Constraint Target Test - CNN-LSTM (No Regularization)     | 0.5867   | 0.5855            | 0.5809        | 0.5780        |  \n",
    "| Zero-Shot: Constraint Target Validation - CNN-LSTM (No Regularization, Multi-Head Attention) | 0.4662   | 0.4671            | 0.4833        | 0.4026        |  \n",
    "| Zero-Shot: Constraint Target Test - CNN-LSTM (No Regularization, Multi-Head Attention)     | 0.4590   | 0.4559            | 0.4763        | 0.3995        |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c757e6c4-961b-485a-9248-23a4646d1cb6",
   "metadata": {},
   "source": [
    "## Zero-Shot: PolitiFact Target Dataset  \n",
    "\n",
    "| Dataset Name + Model Type                                | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|---------------------------------------------------------|----------|-------------------|----------------|----------------|  \n",
    "| **Zero-Shot: PolitiFact Target Validation - CNN-LSTM (L2 Regularization)** | **0.7596**   | **0.7904**            | **0.7328**         | **0.7362**         |  \n",
    "| Zero-Shot: PolitiFact Target Test - CNN-LSTM (L2 Regularization)     | 0.5952   | 0.6427            | 0.6037         | 0.5692         |  \n",
    "| **Zero-Shot: PolitiFact Target Validation - CNN-LSTM (L1 Regularization)** | **0.7596**   | **0.7904**            | **0.7328**         | **0.7362**         |  \n",
    "| Zero-Shot: PolitiFact Target Test - CNN-LSTM (L1 Regularization)     | 0.5873   | 0.6353            | 0.5960         | 0.5588         |  \n",
    "| Zero-Shot: PolitiFact Target Validation - CNN-LSTM (No Regularization) | 0.5577   | 0.5892            | 0.5812         | 0.5536         |  \n",
    "| Zero-Shot: PolitiFact Target Test - CNN-LSTM (No Regularization)     | 0.5397   | 0.5383            | 0.5352         | 0.5278         |  \n",
    "| Zero-Shot: PolitiFact Target Validation - CNN-LSTM (No Regularization, Multi-Head Attention) | 0.6538   | 0.6487            | 0.6501         | 0.6492         |  \n",
    "| **Zero-Shot: PolitiFact Target Test - CNN-LSTM (No Regularization, Multi-Head Attention)**     | **0.6429**   | **0.6429**            | **0.6412**         | **0.6410**         |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f710dc-5340-4629-9e2f-740da03288fc",
   "metadata": {},
   "source": [
    "## Zero-Shot: GossipCop Target Dataset  \n",
    "\n",
    "| Dataset Name + Model Type                                           | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|--------------------------------------------------------------------|----------|-------------------|----------------|----------------|  \n",
    "| Zero-Shot: GossipCop Target Validation - CNN-LSTM (L2 Regularization) | 0.2616   | 0.4982            | 0.4995         | 0.2375         |  \n",
    "| Zero-Shot: GossipCop Target Test - CNN-LSTM (L2 Regularization)      | 0.2701   | 0.5214            | 0.5063         | 0.2456         |  \n",
    "| Zero-Shot: GossipCop Target Validation - CNN-LSTM (L1 Regularization) | 0.4106   | 0.5427            | 0.5471         | 0.4098         |  \n",
    "| Zero-Shot: GossipCop Target Test - CNN-LSTM (L1 Regularization)     | 0.4248   | 0.5446            | 0.5508         | 0.4232         |  \n",
    "| **Zero-Shot: GossipCop Target Validation - CNN-LSTM (No Regularization)** | **0.4673**   | **0.5362**            | **0.5476**         | **0.4562**         |  \n",
    "| Zero-Shot: GossipCop Target Test - CNN-LSTM (No Regularization)     | 0.4717   | 0.5323            | 0.5426         | 0.4592         |  \n",
    "| Zero-Shot: GossipCop Target Validation - CNN-LSTM (No Regularization, Multi-Head Attention) | 0.4635   | 0.5336            | 0.5440         | 0.4527         |  \n",
    "| **Zero-Shot: GossipCop Target Test - CNN-LSTM (No Regularization, Multi-Head Attention)**     | **0.4798**   | **0.5365**            | **0.5484**         | **0.4662**         |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbefde1-4538-4241-b3db-e588aafe4256",
   "metadata": {},
   "source": [
    "## Best Results: Zero-Shot Evaluation Scenario\n",
    "\n",
    "| Dataset Name + Model Type                                           | Accuracy | Precision (Macro) | Recall (Macro) | Macro F1-score |  \n",
    "|--------------------------------------------------------------------|----------|-------------------|----------------|----------------|  \n",
    "| Zero-Shot: WELFake Target Validation - CNN-LSTM (No Regularization) | 0.5817   | 0.5684            | 0.5682        | 0.5683        |  \n",
    "| Zero-Shot: WELFake Target Test - CNN-LSTM (No Regularization)     | 0.5926   | 0.5711            | 0.5725        | 0.5715        |  \n",
    "| Zero-Shot: Constraint Target Validation - CNN-LSTM (L1 Regularization) | 0.5771   | 0.5759            | 0.5759        | 0.5759        |\n",
    "| Zero-Shot: Constraint Target Test - CNN-LSTM (L2 Regularization)     | 0.5895   | 0.5887            | 0.5834        | 0.5800        |    \n",
    "| Zero-Shot: PolitiFact Target Validation - CNN-LSTM (Both L2 and L1 Regularization) | 0.7596   | 0.7904            | 0.7328         | 0.7362         |  \n",
    "| Zero-Shot: PolitiFact Target Test - CNN-LSTM (No Regularization, Multi-Head Attention)     | 0.6429   | 0.6429            | 0.6412         | 0.6410         |  \n",
    "| Zero-Shot: GossipCop Target Validation - CNN-LSTM (No Regularization) | 0.4673   | 0.5362            | 0.5476         | 0.4562         |  \n",
    "| Zero-Shot: GossipCop Target Test - CNN-LSTM (No Regularization, Multi-Head Attention)     | 0.4798   | 0.5365            | 0.5484         | 0.4662         |  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
